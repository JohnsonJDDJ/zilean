{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Crawler\n",
    "\n",
    "This notebook is responsible for data collection process. The notebook is a crawler for high elo League of Legends matches. Crawling is achieved through the Riot Development API and the package `riotwatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from riotwatcher import LolWatcher, ApiError\n",
    "from tqdm.notebook import tqdm\n",
    "import zilean\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "api_key = zilean.read_api_key()\n",
    "watcher = LolWatcher(api_key=api_key)\n",
    "REGION = 'kr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Challenger League\n",
    "\n",
    "Fetch the summoner's in challenger league of Korea server and cache it locally to `data/kr_challenger_league.json`. The last time it was cached is in 2022-05-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/kr_challenger_league.json'):\n",
    "    with open('data/kr_challenger_league.json', 'r') as f:\n",
    "        kr_challenger_league = json.load(f)\n",
    "    print(\"Loaded previously cached data.\")\n",
    "else :\n",
    "    kr_challenger_league = watcher.league.challenger_by_queue(REGION, \"RANKED_SOLO_5x5\")\n",
    "    with open('data/kr_challenger_league.json', 'w') as f:\n",
    "        json.dump(kr_challenger_league, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the `puuid` for each challenger account. This is because we can only access matches using `puuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached accounts\n"
     ]
    }
   ],
   "source": [
    "# List of challenger summonerIds\n",
    "challengers = [entry['summonerId'] for entry in kr_challenger_league['entries']]\n",
    "\n",
    "# Fetch puuid for each summonerId\n",
    "if os.path.exists('data/accounts.json'):\n",
    "    with open('data/accounts.json', 'r') as f:\n",
    "        accounts = json.load(f)\n",
    "        print(\"Loaded previously cached accounts\")\n",
    "else:\n",
    "    # Clean dirty files resulting from previous search\n",
    "    if os.path.exists('data/accounts.txt'):\n",
    "        os.remove('data/accounts.txt')\n",
    "    # Iterate over summonerIds\n",
    "    for i, challenger in enumerate(challengers):\n",
    "        account = watcher.summoner.by_id(REGION, challenger)\n",
    "        with open('data/accounts.txt', 'a') as f:\n",
    "            f.write(str(account))\n",
    "        print(f\"Counter: {i}.\")\n",
    "\n",
    "    with open('data/accounts.txt', 'r') as f:\n",
    "            accounts = f.read()\n",
    "\n",
    "    accounts = re.sub('}{', '}@@@{', accounts)\n",
    "    accounts = accounts.split('@@@')\n",
    "\n",
    "    with open('data/accounts.json', 'w') as f:\n",
    "        json.dump(accounts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch High Elo Matches and Timelines\n",
    "\n",
    "Fetch the recent matches and timelines by the `puuid` of each challenger account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached matches\n"
     ]
    }
   ],
   "source": [
    "MATCH_PER_PUUID = 5\n",
    "visited_matchids = set()\n",
    "\n",
    "# Fetch match for each PUUID\n",
    "if os.path.exists('data/matches.json'):\n",
    "    with open('data/matches.json', 'r') as f:\n",
    "        matches = json.load(f)\n",
    "        print(\"Loaded previously cached matches\")\n",
    "else :\n",
    "    # Iterate over accounts (n=300)\n",
    "    for account_index, account in enumerate(tqdm(accounts)):\n",
    "        # Prepare string to JSON\n",
    "        account = account.replace(\"\\'\", \"\\\"\")\n",
    "        account = json.loads(account)\n",
    "        matchids = watcher.match.matchlist_by_puuid(REGION, account['puuid'])\n",
    "        # Iterate over recent matches (n=MATCH_PER_PUUID)\n",
    "        for match_index in range(MATCH_PER_PUUID):\n",
    "            matchid = matchids[match_index]\n",
    "            # Skip over visited matches\n",
    "            if matchid in visited_matchids: continue\n",
    "            timeline = watcher.match.timeline_by_match(REGION, matchid)\n",
    "            matchinfo = {\"id\": matchid, \"timeline\": timeline}\n",
    "            zilean.write_messy_json(matchinfo, \"data/matches.json\")\n",
    "            visited_matchids.add(matchid)\n",
    "            # print(f\"Counter: {account_index * MATCH_PER_PUUID + match_index}.\")\n",
    "\n",
    "    zilean.clean_json(\"data/matches.json\")\n",
    "    with open('data/matches.json', 'r') as f:\n",
    "        matches = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup and store to JSON\n",
    "\n",
    "The files `matches.json` may contain some unwanted matches. We iterate through each match and only retain matches that is longer than 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 787 crawled KR high elo matches longer than 16 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Minimum duration (in minutes) of a match\n",
    "min_duration = 16\n",
    "\n",
    "# Create a mask to filter out short matches\n",
    "long_matchids = [True for i in range(len(matches))]\n",
    "for i, match in enumerate(matches):\n",
    "    frame_interval = match['timeline']['info']['frameInterval']\n",
    "    if len(match['timeline']['info']['frames']) < int(min_duration * 60000 / frame_interval):\n",
    "        long_matchids[i] = False\n",
    "matches = [match for match, is_long in zip(matches, long_matchids) if is_long]\n",
    "print(f\"There are in total {len(matches)} crawled KR high elo matches longer than {min_duration} minutes.\")\n",
    "\n",
    "# Save to disk\n",
    "with open('data/matches_cleaned.json', 'w') as f:\n",
    "    json.dump(matches, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "759a339a8a10cd21f51e8d880db49e879732e80ae0888603c78477bf3bed2c39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('league')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
