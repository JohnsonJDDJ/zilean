{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Crawler\n",
    "\n",
    "This notebook is responsible for data collection process. The notebook is a crawler for high elo League of Legends matches. Crawling is achieved through the Riot Development API and the package `riotwatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from riotwatcher import LolWatcher, ApiError\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "api_key = 'RGAPI-28637d7d-49ae-49f1-993f-e6a41359ff1e'\n",
    "watcher = LolWatcher(api_key=api_key)\n",
    "REGION = 'kr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Challenger League\n",
    "\n",
    "Fetch the summoner's in challenger league of Korea server and cache it locally to `data/kr_challenger_league.json`. The last time it was cached is in 2022-05-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/kr_challenger_league.json'):\n",
    "    with open('data/kr_challenger_league.json', 'r') as f:\n",
    "        kr_challenger_league = json.load(f)\n",
    "    print(\"Loaded previously cached data.\")\n",
    "else :\n",
    "    kr_challenger_league = watcher.league.challenger_by_queue(REGION, \"RANKED_SOLO_5x5\")\n",
    "    with open('data/kr_challenger_league.json', 'w') as f:\n",
    "        json.dump(kr_challenger_league, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the `puuid` for each challenger account. This is because we can only access matches using `puuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached accounts\n"
     ]
    }
   ],
   "source": [
    "# List of challenger summonerIds\n",
    "challengers = [entry['summonerId'] for entry in kr_challenger_league['entries']]\n",
    "\n",
    "# Fetch puuid for each summonerId\n",
    "if os.path.exists('data/accounts.json'):\n",
    "    with open('data/accounts.json', 'r') as f:\n",
    "        accounts = json.load(f)\n",
    "        print(\"Loaded previously cached accounts\")\n",
    "else:\n",
    "    # Clean dirty files resulting from previous search\n",
    "    if os.path.exists('data/accounts.txt'):\n",
    "        os.remove('data/accounts.txt')\n",
    "    # Iterate over summonerIds\n",
    "    for i, challenger in enumerate(challengers):\n",
    "        account = watcher.summoner.by_id(REGION, challenger)\n",
    "        with open('data/accounts.txt', 'a') as f:\n",
    "            f.write(str(account))\n",
    "        print(f\"Counter: {i}.\")\n",
    "\n",
    "    with open('data/accounts.txt', 'r') as f:\n",
    "            accounts = f.read()\n",
    "\n",
    "    accounts = re.sub('}{', '}@@@{', accounts)\n",
    "    accounts = accounts.split('@@@')\n",
    "\n",
    "    with open('data/accounts.json', 'w') as f:\n",
    "        json.dump(accounts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch High Elo Matches and Timelines\n",
    "\n",
    "Fetch the recent matches and timelines by the `puuid` of each challenger account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached matches\n",
      "Loaded previously cached matches\n"
     ]
    }
   ],
   "source": [
    "MATCH_PER_PUUID = 5\n",
    "visited_matchids = set()\n",
    "\n",
    "# Fetch match for each PUUID\n",
    "if os.path.exists('data/matches.json') and os.path.exists('data/timelines.json'):\n",
    "    with open('data/matches.json', 'r') as f:\n",
    "        matches = json.load(f)\n",
    "        print(\"Loaded previously cached matches\")\n",
    "    with open('data/timelines.json', 'r') as f:\n",
    "        timelines = json.load(f)\n",
    "        print(\"Loaded previously cached matches\")\n",
    "\n",
    "elif not os.path.exists('data/matches.json') and not os.path.exists('data/timelines.json'):\n",
    "\n",
    "    # Clean dirty files resulting from previous search\n",
    "    if os.path.exists('data/matches.txt'):\n",
    "        os.remove('data/matches.txt')\n",
    "    if os.path.exists('data/timelines.txt'):\n",
    "        os.remove('data/timelines.txt')  \n",
    "\n",
    "    # Iterate over accounts (n=300)\n",
    "    for account_index, account in enumerate(accounts):\n",
    "        # Prepare string to JSON\n",
    "        account = account.replace(\"\\'\", \"\\\"\")\n",
    "        account = json.loads(account)\n",
    "        matchids = watcher.match.matchlist_by_puuid(REGION, account['puuid'])\n",
    "        # Iterate over recent matches (n=MATCH_PER_PUUID)\n",
    "        for match_index in range(MATCH_PER_PUUID):\n",
    "            matchid = matchids[match_index]\n",
    "            # Skip over visited matches\n",
    "            if matchid in visited_matchids: continue\n",
    "            match = watcher.match.by_id(REGION, matchid)\n",
    "            timeline = watcher.match.timeline_by_match(REGION, matchid)\n",
    "            with open('data/matches.txt', 'a') as f:\n",
    "                f.write(str(match))\n",
    "                f.write(\"@@@@@@\")\n",
    "            with open('data/timelines.txt', 'a') as f:\n",
    "                f.write(str(timeline))\n",
    "                f.write(\"@@@@@@\")\n",
    "            visited_matchids.add(matchid)\n",
    "            print(f\"Counter: {account_index * MATCH_PER_PUUID + match_index}.\")\n",
    "\n",
    "    # Save crawled matches to disk\n",
    "    with open('data/matches.txt', 'r') as f:\n",
    "        matches = f.read()  \n",
    "    matches = matches.split('@@@@@@')\n",
    "    with open('data/matches.json', 'w') as f:\n",
    "        json.dump(matches, f)\n",
    "\n",
    "    # Save crawled timelines to disk\n",
    "    with open('data/timelines.txt', 'r') as f:\n",
    "        timelines = f.read()  \n",
    "    timelines = timelines.split('@@@@@@')\n",
    "    with open('data/timelines.json', 'w') as f:\n",
    "        json.dump(timelines, f)\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch files, please check data directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "759a339a8a10cd21f51e8d880db49e879732e80ae0888603c78477bf3bed2c39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('league')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
