{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Crawler\n",
    "\n",
    "This notebook is responsible for data collection process. The notebook is a crawler for high elo League of Legends matches. Crawling is achieved through the Riot Development API and the package `riotwatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from riotwatcher import LolWatcher, ApiError\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "api_key = 'RGAPI-28637d7d-49ae-49f1-993f-e6a41359ff1e'\n",
    "watcher = LolWatcher(api_key=api_key)\n",
    "REGION = 'kr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Challenger League\n",
    "\n",
    "Fetch the summoner's in challenger league of Korea server and cache it locally to `data/kr_challenger_league.json`. The last time it was cached is in 2022-05-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/kr_challenger_league.json'):\n",
    "    with open('data/kr_challenger_league.json', 'r') as f:\n",
    "        kr_challenger_league = json.load(f)\n",
    "    print(\"Loaded previously cached data.\")\n",
    "else :\n",
    "    kr_challenger_league = watcher.league.challenger_by_queue(REGION, \"RANKED_SOLO_5x5\")\n",
    "    with open('data/kr_challenger_league.json', 'w') as f:\n",
    "        json.dump(kr_challenger_league, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the `puuid` for each challenger account. This is because we can only access matches using `puuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously cached accounts\n"
     ]
    }
   ],
   "source": [
    "# List of challenger summonerIds\n",
    "challengers = [entry['summonerId'] for entry in kr_challenger_league['entries']]\n",
    "\n",
    "# Fetch puuid for each summonerId\n",
    "if os.path.exists('data/accounts.json'):\n",
    "    with open('data/accounts.json', 'r') as f:\n",
    "        accounts = json.load(f)\n",
    "        print(\"Loaded previously cached accounts\")\n",
    "else:\n",
    "    # Clean dirty files resulting from previous search\n",
    "    if os.path.exists('data/accounts.txt'):\n",
    "        os.remove('data/accounts.txt')\n",
    "    # Iterate over summonerIds\n",
    "    for i, challenger in enumerate(challengers):\n",
    "        account = watcher.summoner.by_id(REGION, challenger)\n",
    "        with open('data/accounts.txt', 'a') as f:\n",
    "            f.write(str(account))\n",
    "        print(f\"Counter: {i}.\")\n",
    "\n",
    "    with open('data/accounts.txt', 'r') as f:\n",
    "            accounts = f.read()\n",
    "\n",
    "    accounts = re.sub('}{', '}@@@{', accounts)\n",
    "    accounts = accounts.split('@@@')\n",
    "\n",
    "    with open('data/accounts.json', 'w') as f:\n",
    "        json.dump(accounts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch High Elo Matches and Timelines\n",
    "\n",
    "Fetch the recent matches and timelines by the `puuid` of each challenger account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb#ch0000009?line=25'>26</a>\u001b[0m matchids \u001b[39m=\u001b[39m watcher\u001b[39m.\u001b[39mmatch\u001b[39m.\u001b[39mmatchlist_by_puuid(REGION, account[\u001b[39m'\u001b[39m\u001b[39mpuuid\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb#ch0000009?line=26'>27</a>\u001b[0m \u001b[39m# Iterate over recent matches (n=MATCH_PER_PUUID)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb#ch0000009?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m match_index \u001b[39min\u001b[39;00m MATCH_PER_PUUID:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb#ch0000009?line=28'>29</a>\u001b[0m     matchid \u001b[39m=\u001b[39m matchids[match_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/JDDJ/Documents/Self/Programming/leagueoflegends/zilean/match_crawler.ipynb#ch0000009?line=29'>30</a>\u001b[0m     \u001b[39m# Skip over visited matches\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "MATCH_PER_PUUID = 1\n",
    "visited_matchids = set()\n",
    "\n",
    "# Fetch match for each PUUID\n",
    "if os.path.exists('data/matches.json') and os.path.exists('data/timelines.json'):\n",
    "    with open('data/matches.json', 'r') as f:\n",
    "        matches = json.load(f)\n",
    "        print(\"Loaded previously cached matches\")\n",
    "    with open('data/timelines.json', 'r') as f:\n",
    "        timelines = json.load(f)\n",
    "        print(\"Loaded previously cached matches\")\n",
    "\n",
    "elif not os.path.exists('data/matches.json') and not os.path.exists('data/timelines.json'):\n",
    "\n",
    "    # Clean dirty files resulting from previous search\n",
    "    if os.path.exists('data/matches.txt'):\n",
    "        os.remove('data/matches.txt')\n",
    "    if os.path.exists('data/timelines.txt'):\n",
    "        os.remove('data/timelines.txt')  \n",
    "\n",
    "    # Iterate over accounts (n=300)\n",
    "    for account_index, account in enumerate(accounts):\n",
    "        # Prepare string to JSON\n",
    "        account = account.replace(\"\\'\", \"\\\"\")\n",
    "        account = json.loads(account)\n",
    "        matchids = watcher.match.matchlist_by_puuid(REGION, account['puuid'])\n",
    "        # Iterate over recent matches (n=MATCH_PER_PUUID)\n",
    "        for match_index in range(MATCH_PER_PUUID):\n",
    "            matchid = matchids[match_index]\n",
    "            # Skip over visited matches\n",
    "            if matchid in visited_matchids: continue\n",
    "            match = watcher.match.by_id(REGION, matchid)\n",
    "            timeline = watcher.match.timeline_by_match(REGION, matchid)\n",
    "            with open('data/matches.txt', 'a') as f:\n",
    "                f.write(str(match))\n",
    "                f.write(\"@@@@@@\")\n",
    "            with open('data/timelines.txt', 'a') as f:\n",
    "                f.write(str(timeline))\n",
    "                f.write(\"@@@@@@\")\n",
    "            visited_matchids.add(matchid)\n",
    "            print(f\"Counter: {account_index * MATCH_PER_PUUID + match_index}.\")\n",
    "\n",
    "    # Save crawled matches to disk\n",
    "    with open('data/matches.txt', 'r') as f:\n",
    "        matches = f.read()  \n",
    "    matches = matches.split('@@@@@@')\n",
    "    with open('data/matches.json', 'w') as f:\n",
    "        json.dump(MATCH_PER_PUUID, f)\n",
    "\n",
    "    # Save crawled timelines to disk\n",
    "    with open('data/timelines.txt', 'r') as f:\n",
    "        timelines = f.read()  \n",
    "    timelines = timelines.split('@@@@@@')\n",
    "    with open('data/timelines.json', 'w') as f:\n",
    "        json.dump(timelines, f)\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch files, please check data directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'eZSUIU09ORLZonrW_DdxTsPDch_S8u0-i3foyt2ahl8ma1_-DdR_crmDjA',\n",
       " 'accountId': 'HbvN6-lt4ao_4Ps2_ed0V7kBoFGOFlOaoSoKTiYTTJJaA5o5yVIe30zV',\n",
       " 'puuid': 'x2IpDOTbQi8g9NwT53TFTk2Flg5qO9Pj0JthidWeAl6D2_UNmWYKGbSzXQTz03lgG7_Q6VS4jJ8Fdw',\n",
       " 'name': '순삭쿠키',\n",
       " 'profileIconId': 5212,\n",
       " 'revisionDate': 1652510405000,\n",
       " 'summonerLevel': 67}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(account)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "759a339a8a10cd21f51e8d880db49e879732e80ae0888603c78477bf3bed2c39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('league')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
